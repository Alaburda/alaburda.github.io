[
  {
    "objectID": "posts/grouping-overlapping-date-ranges.html",
    "href": "posts/grouping-overlapping-date-ranges.html",
    "title": "Grouping Overlapping Date Ranges",
    "section": "",
    "text": "It’s pretty straightforward to group rows in SQL if their date ranges are consecutive or have fixed gaps.1 It’s much harder if you want to group overlapping date ranges. I personally haven’t used this trick in my own practice but there are use cases for it. For example, if a chat agent can work with multiple clients at once, your chat sessions will overlap. But you can’t just aggregate the duration of the chats if you want to estimate the agent active time. That would give you an overestimate! Generally, grouping overlapping date ranges is a useful trick for whenever you want to know “active hours”."
  },
  {
    "objectID": "posts/grouping-overlapping-date-ranges.html#sample-data",
    "href": "posts/grouping-overlapping-date-ranges.html#sample-data",
    "title": "Grouping Overlapping Date Ranges",
    "section": "Sample Data",
    "text": "Sample Data\nThe sample data are randomly generated date intervals (i.e. they have a start date and an end date), generated in a way so that each colored group’s intervals overlap most of the time."
  },
  {
    "objectID": "posts/grouping-overlapping-date-ranges.html#sql-solution-step-by-step",
    "href": "posts/grouping-overlapping-date-ranges.html#sql-solution-step-by-step",
    "title": "Grouping Overlapping Date Ranges",
    "section": "SQL Solution Step-by-Step",
    "text": "SQL Solution Step-by-Step\nThe following SQL code works in four steps and let’s imagine we’re an interval in this dataset:\n\nFirst of all, the BETWEEN UNBOUNDED PRECEDING and 1 PRECEDING is the key to this trick. For every row, it answers the question “which of the intervals that started before me have the latest end date?”.\n\n\n\nwith prev_date as (\nSELECT\n  ROW_NUMBER() OVER (ORDER BY group_id, date_from, date_to) AS date_rank,\n  group_id,\n  date_from,\n  date_to,\n  MAX(date_to) OVER (PARTITION BY group_id ORDER BY date_from, date_to ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS previous_date_to\nFROM\n  overlapping_ranges\n)\nselect \n  *\nfrom prev_date\norder by group_id, date_from\n\n\nDisplaying records 1 - 10\n\n\ndate_rank\ngroup_id\ndate_from\ndate_to\nprevious_date_to\n\n\n\n\n1\n1\n2024-07-01\n2024-07-10\nNA\n\n\n2\n1\n2024-07-13\n2024-07-20\n2024-07-10\n\n\n3\n1\n2024-07-14\n2024-07-15\n2024-07-20\n\n\n4\n1\n2024-07-16\n2024-07-19\n2024-07-20\n\n\n5\n1\n2024-07-16\n2024-07-26\n2024-07-20\n\n\n6\n1\n2024-07-25\n2024-08-01\n2024-07-26\n\n\n7\n1\n2024-07-28\n2024-07-29\n2024-08-01\n\n\n8\n1\n2024-07-30\n2024-08-03\n2024-08-01\n\n\n9\n1\n2024-07-30\n2024-08-05\n2024-08-03\n\n\n10\n1\n2024-07-31\n2024-08-01\n2024-08-05\n\n\n\n\n\n\nThen, we check whether the interval before me ended before I had started or after I had started.\n\n\n\nwith prev_date as (\nSELECT\n  ROW_NUMBER() OVER (ORDER BY group_id, date_from, date_to) AS date_rank,\n  group_id,\n  date_from,\n  date_to,\n  MAX(date_to) OVER (PARTITION BY group_id ORDER BY date_from, date_to ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS previous_date_to\nFROM\n  overlapping_ranges\n),\nislands as (\nSELECT\n *,\n CASE WHEN previous_date_to &gt;= date_from THEN 0 ELSE 1 END AS island_start_id\nfrom prev_date\n)\nselect \n  *\nfrom islands\norder by group_id, date_from\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\ndate_rank\ngroup_id\ndate_from\ndate_to\nprevious_date_to\nisland_start_id\n\n\n\n\n1\n1\n2024-07-01\n2024-07-10\nNA\n1\n\n\n2\n1\n2024-07-13\n2024-07-20\n2024-07-10\n1\n\n\n3\n1\n2024-07-14\n2024-07-15\n2024-07-20\n0\n\n\n4\n1\n2024-07-16\n2024-07-19\n2024-07-20\n0\n\n\n5\n1\n2024-07-16\n2024-07-26\n2024-07-20\n0\n\n\n6\n1\n2024-07-25\n2024-08-01\n2024-07-26\n0\n\n\n7\n1\n2024-07-28\n2024-07-29\n2024-08-01\n0\n\n\n8\n1\n2024-07-30\n2024-08-03\n2024-08-01\n0\n\n\n9\n1\n2024-07-30\n2024-08-05\n2024-08-03\n0\n\n\n10\n1\n2024-07-31\n2024-08-01\n2024-08-05\n0\n\n\n\n\n\n\nFinally, we run a rolling SUM over the second step to find consecutive overlaps. Essentially, if the interval before me overlapped with someone, if I’m overlapping with the interval before me, and the interval after me overlapping with someone, that is good enough to say that yes, all of these three intervals form a chain of overlapping intervals.\n\n\n\nwith prev_date as (\nSELECT\n  ROW_NUMBER() OVER (ORDER BY group_id, date_from, date_to) AS date_rank,\n  group_id,\n  date_from,\n  date_to,\n  MAX(date_to) OVER (PARTITION BY group_id ORDER BY date_from, date_to ROWS BETWEEN UNBOUNDED PRECEDING AND 1 PRECEDING) AS previous_date_to\nFROM\n  overlapping_ranges\n),\nislands as (\nSELECT\n *,\n CASE WHEN previous_date_to &gt;= date_from THEN 0 ELSE 1 END AS island_start_id,\n SUM(CASE WHEN previous_date_to &gt;= date_from THEN 0 ELSE 1 END) OVER (ORDER BY date_rank) AS island_id\nfrom prev_date\n)\nselect \n  *\nfrom islands\norder by group_id, date_from\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\ndate_rank\ngroup_id\ndate_from\ndate_to\nprevious_date_to\nisland_start_id\nisland_id\n\n\n\n\n1\n1\n2024-07-01\n2024-07-10\nNA\n1\n1\n\n\n2\n1\n2024-07-13\n2024-07-20\n2024-07-10\n1\n2\n\n\n3\n1\n2024-07-14\n2024-07-15\n2024-07-20\n0\n2\n\n\n4\n1\n2024-07-16\n2024-07-19\n2024-07-20\n0\n2\n\n\n5\n1\n2024-07-16\n2024-07-26\n2024-07-20\n0\n2\n\n\n6\n1\n2024-07-25\n2024-08-01\n2024-07-26\n0\n2\n\n\n7\n1\n2024-07-28\n2024-07-29\n2024-08-01\n0\n2\n\n\n8\n1\n2024-07-30\n2024-08-03\n2024-08-01\n0\n2\n\n\n9\n1\n2024-07-30\n2024-08-05\n2024-08-03\n0\n2\n\n\n10\n1\n2024-07-31\n2024-08-01\n2024-08-05\n0\n2\n\n\n\n\n\n\nAnd of course, we aggregate on the result of the rolling of SUM to get the start and end dates of the whole overlapping cluster of rows."
  },
  {
    "objectID": "posts/grouping-overlapping-date-ranges.html#footnotes",
    "href": "posts/grouping-overlapping-date-ranges.html#footnotes",
    "title": "Grouping Overlapping Date Ranges",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBlog posts pending!↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\nGrouping Overlapping Date Ranges\n\n\n\nSQL\n\n\n\n\n\n\n\nPaulius Alaburda\n\n\nSep 13, 2024\n\n\n\n\n\n\n\n\n\n\n\nGrouping Non-Overlapping Date Ranges\n\n\n\nSQL\n\n\n\n\n\n\n\nPaulius Alaburda\n\n\nSep 22, 2024\n\n\n\n\n\n\n\n\n\n\n\ndbt docs runs queries with pivot macros?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndbt anti-pattern: using {{ this }} directly in incremental refreshes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Paulius Alaburda",
    "section": "",
    "text": "Hello there! I’m Paulius Alaburda, I’m a data analyst with a background in energy, statistics, medicine, biology and insurance. Having finished medschool in 2018, I pursued a career in data - and to be fair, I never looked back! I believe a great data analyst is someone who understands the code they write or the tools they use but also has the ability to explain their results to someone."
  },
  {
    "objectID": "talks/2024-05-16-i-love-duckdb.html",
    "href": "talks/2024-05-16-i-love-duckdb.html",
    "title": "I Love DuckDB",
    "section": "",
    "text": "This was a talk at the first ever Business Garden R User Group in 2024.\nhttps://alaburda.github.io/i-love-duckdb/#/title-slide"
  },
  {
    "objectID": "posts/grouping-nonoverlapping-date-ranges.html",
    "href": "posts/grouping-nonoverlapping-date-ranges.html",
    "title": "Grouping Non-Overlapping Date Ranges",
    "section": "",
    "text": "If you’re a data analyst at a company, it’s VERY likely your company stores some data as a Type 2 slowly-changing dimension. It’s also likely that the way it’s stored may not align with how you want to model it in the warehouse. Imagine you’re managing a subscription-based service. You might want to display a user’s subscription history as a single entry, showing the start date of their first subscription and the end date of their last - but only if there were no gaps in service. Or perhaps you’re tracking subscription tiers, and you want to combine multiple entries into one as long as the tier remained the same. In this post, we’ll explore how to handle these scenarios efficiently, making your data more manageable and insightful.\nAlso, as an aside, I’ve started this post as a “consecutive” date range blog post for cases where your rows are consecutive and gapless. But in reality, the island and gaps method is universal for any scenario where the date ranges are not overlapping."
  },
  {
    "objectID": "posts/grouping-nonoverlapping-date-ranges.html#sample-data",
    "href": "posts/grouping-nonoverlapping-date-ranges.html#sample-data",
    "title": "Grouping Non-Overlapping Date Ranges",
    "section": "Sample Data",
    "text": "Sample Data\nLet’s follow along with the example I introduced just now: our sample data are consecutive date ranges that alternate between two subscription tiers (highlighted with a different color). Let’s also introduce small gaps between the date ranges - visually they’re not visible but the SQL query will pick up on them.\n\nlibrary(duckdb)\n\nLoading required package: DBI\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(ggplot2)\n\ncon &lt;- dbConnect(duckdb::duckdb())\n\nsource(\"functions.R\")\n\nconsecutive_dates &lt;- generate_intervals(groups = 3, duration = 20, gaps = 3, n = 20) %&gt;% \n  mutate(tier = sample(c(\"Free\",\"Premium\"), size = 20, replace = TRUE, prob = c(0.8,0.2)))\n\np1 &lt;- consecutive_dates %&gt;% \n  plot_timeline() + \n  geom_point(aes(color = tier)) +\n  geom_line(aes(color = tier)) +\n  facet_grid(~group_id) + \n  labs(title = \"Non-overlapping intervals alternating between two subscription tiers.\")\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\np1\n\n\n\n\n\n\n\ndbWriteTable(con, \"consecutive_dates\", consecutive_dates, overwrite = TRUE)\n\nI’m interested in two scenarios. First, grouping these intervals by their tier if the intervals are consecutive. Second, I want to also group those intervals by tier as well as by whether they were uninterrupted. I’m not interested in JUST getting the start and end dates for each group - for that, a normal group by would suffice."
  },
  {
    "objectID": "posts/grouping-nonoverlapping-date-ranges.html#grouping-date-ranges-by-group-and-tier",
    "href": "posts/grouping-nonoverlapping-date-ranges.html#grouping-date-ranges-by-group-and-tier",
    "title": "Grouping Non-Overlapping Date Ranges",
    "section": "Grouping date ranges by group and tier",
    "text": "Grouping date ranges by group and tier\nAll of the queries will follow the same pattern:\n\nFirst, we do a row_number() for our groups whose rows are chronological - essentially, we’re building an index for each of our groups;\nThen, we create another row_number() column that uses the group variable as well as ALL the rest of the columns we want to group by\nFinally, we group by the difference between the two columns. The idea is that if the first and the second row_numbers are increasing sequentally at the same rate, their difference will produce a constant value. What’s even better is that value will be unique within the scope of the columns we’re grouping by!\n\n\n\nwith tmp as (\n  select\n    row_number() over (partition by group_id order by date_from) as group_rank,\n    row_number() over (partition by group_id, tier order by date_from) as tier_rank,\n    group_id,\n    tier,\n    date_from,\n    date_to\n  from consecutive_dates\n)\nselect\n  group_id,\n  tier,\n  min(date_from) as date_from,\n  max(date_to) as date_to\nfrom tmp\ngroup by group_id, tier, (group_rank-tier_rank)\n\n\nislands &lt;- islands %&gt;% \n  mutate(island_id = as.character(row_number()))\n\nconsecutive_dates %&gt;% \n  plot_timeline() +\n  facet_grid(~group_id) +\n  geom_rect(data = islands, aes(y = NULL, x = NULL, ymin = date_from, ymax = date_to, xmin = -Inf, xmax = Inf, fill = tier, color = tier), alpha = 0.3) + \n  geom_point(aes(color = tier)) +\n  geom_line(aes(color = tier)) +\n  guides(fill = FALSE)"
  },
  {
    "objectID": "posts/grouping-nonoverlapping-date-ranges.html#amending-the-sql-query-to-allow-multiple-columns",
    "href": "posts/grouping-nonoverlapping-date-ranges.html#amending-the-sql-query-to-allow-multiple-columns",
    "title": "Grouping Non-Overlapping Date Ranges",
    "section": "Amending the SQL query to allow multiple columns",
    "text": "Amending the SQL query to allow multiple columns\nWhat if we also want to check whether the data ranges are uninterrupted, i.e. they have no gaps? We have to build a new variable that checks whether the current subscription followed immediately after the one before it (through lag(date_to,1)). Then, we have to partition by that new column but we have to create a combined column from tier and the new column is_consecutive.\n\n\nwith lag_date_from as (\n  select \n    consecutive_dates.*,\n    lag(date_to,1) over (partition by group_id order by date_from) as lag_date_to\n  from consecutive_dates\n),\ngap_calc as (\n  select \n    lag_date_from.*,\n    case when lag_date_to + interval 1 day = date_from then 1 else 0 end as is_consecutive\n  from lag_date_from\n),\ntmp as (\n  select\n    row_number() over (partition by group_id order by date_from) as group_rank,\n    row_number() over (partition by group_id, tier order by date_from) as group_tier_rank,\n    row_number() over (partition by group_id, tier||cast(is_consecutive as varchar) order by date_from) as group_tier_cons_rank,\n    tier||cast(is_consecutive as varchar) as tier_cons,\n    group_id,\n    is_consecutive,\n    tier,\n    date_from,\n    date_to\n  from gap_calc\n)\nselect \n  group_id,\n  tier_cons,\n  min(date_from) as date_from,\n  max(date_to) as date_to\nfrom tmp\ngroup by group_id, tier_cons, (group_rank-group_tier_cons_rank)\n\n\nconsecutive_islands &lt;- consecutive_islands %&gt;% \n  mutate(island_id = as.character(row_number()))\n\nconsecutive_dates %&gt;% \n  plot_timeline() +\n  facet_grid(~group_id) +\n  geom_rect(data = consecutive_islands, aes(y = NULL, x = NULL, ymin = date_from, ymax = date_to, xmin = -Inf, xmax = Inf, fill = tier_cons, color = tier_cons), alpha = 0.3) + \n  geom_point(aes(color = tier)) +\n  geom_line(aes(color = tier)) +\n  guides(fill = FALSE)"
  },
  {
    "objectID": "posts/grouping-nonoverlapping-date-ranges.html#the-end---jfc-that-was-an-ugly-plot",
    "href": "posts/grouping-nonoverlapping-date-ranges.html#the-end---jfc-that-was-an-ugly-plot",
    "title": "Grouping Non-Overlapping Date Ranges",
    "section": "The End - jfc that was an ugly plot",
    "text": "The End - jfc that was an ugly plot\nIt was! I don’t mind, though - it’s there to visually prove the query works. The query is extensible to other scenarios. For example, if you had a task of grouping user events into sessions if the events happened within 3 days of each other, you could just change the case when statement that creates the is_consecutive column so that we check whether lag(date_to,1) + interval 3 day is equal to or larger than date_from. Feel free to try the pattern out yourself!"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\nI Love DuckDB\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dbt-anti-pattern-using-this.html",
    "href": "posts/dbt-anti-pattern-using-this.html",
    "title": "dbt anti-pattern: using {{ this }} directly in incremental refreshes",
    "section": "",
    "text": "If you’re using dbt, the recommended default way to build incremental models looks like this:\n{{\n    config(\n        materialized='incremental'\n    )\n}}\n\nselect\n    *,\n    my_slow_function(my_column)\n\nfrom {{ ref('app_data_events') }}\n\n{% if is_incremental() %}\n\n  -- this filter will only be applied on an incremental run\n  -- (uses &gt;= to include records whose timestamp occurred since the last run of this model)\n  -- (If event_time is NULL or the table is truncated, the condition will always be true and load all records)\nwhere event_time &gt;= (select coalesce(max(event_time),'1900-01-01') from {{ this }} )\n\n{% endif %}\nIf you’re unfamiliar with dbt’s Jinja syntax, { this } refers to the table that is built incrementally. If the table doesn’t exist yet, dbt ignores the where condition and just runs the query. Afterwards, it checks the freshness of the incremental table via max(event_time). The latest date is passed back to the source data so that we only add new data and avoiding running the query on data that was already processed and saved into the table.\nLet’s run a very similar query in DuckDB using the taxi dataset. This is the query plan :\nlibrary(duckdb)\n\ncon &lt;- dbConnect(duckdb::duckdb())\n\ndbGetQuery(con, \"explain analyze select * from 'taxi_2019_04.parquet' WHERE pickup_at &gt; (select median(pickup_at) from 'taxi_2019_04.parquet')\")\n\nanalyzed_plan\n┌─────────────────────────────────────┐\n│┌───────────────────────────────────┐│\n││    Query Profiling Information    ││\n│└───────────────────────────────────┘│\n└─────────────────────────────────────┘\nexplain analyze select * from 'taxi_2019_04.parquet' WHERE pickup_at &gt; (select median(pickup_at) from 'taxi_2019_04.parquet')\n┌────────────────────────────────────────────────┐\n│┌──────────────────────────────────────────────┐│\n││              Total Time: 0.277s              ││\n│└──────────────────────────────────────────────┘│\n└────────────────────────────────────────────────┘\n┌───────────────────────────┐\n│           QUERY           │\n│    ────────────────────   │\n│           0 Rows          │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│      EXPLAIN_ANALYZE      │\n│    ────────────────────   │\n│           0 Rows          │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│         PROJECTION        │\n│    ────────────────────   │\n│         vendor_id         │\n│         pickup_at         │\n│         dropoff_at        │\n│      passenger_count      │\n│       trip_distance       │\n│        rate_code_id       │\n│     store_and_fwd_flag    │\n│     pickup_location_id    │\n│    dropoff_location_id    │\n│        payment_type       │\n│        fare_amount        │\n│           extra           │\n│          mta_tax          │\n│         tip_amount        │\n│        tolls_amount       │\n│   improvement_surcharge   │\n│        total_amount       │\n│    congestion_surcharge   │\n│                           │\n│        3716563 Rows       │\n│          (0.00s)          │\n└─────────────┬─────────────┘\n┌─────────────┴─────────────┐\n│      NESTED_LOOP_JOIN     │\n│    ────────────────────   │\n│      Join Type: INNER     │\n│                           │\n│        Conditions:        ├──────────────┐\n│    pickup_at &gt; SUBQUERY   │              │\n│                           │              │\n│        3716563 Rows       │              │\n│          (0.05s)          │              │\n└─────────────┬─────────────┘              │\n┌─────────────┴─────────────┐┌─────────────┴─────────────┐\n│         TABLE_SCAN        ││         PROJECTION        │\n│    ────────────────────   ││    ────────────────────   │\n│         Function:         ││ CASE  WHEN ((#1 &gt; 1)) THEN│\n│        PARQUET_SCAN       ││  (error('More than one row│\n│                           ││   returned by a subquery  │\n│        Projections:       ││   used as an expression - │\n│         pickup_at         ││    scalar subqueries can  │\n│         vendor_id         ││  only return a single row.│\n│         dropoff_at        ││          Use \"SET         │\n│      passenger_count      ││ scalar_subquery_error_on_m│\n│       trip_distance       ││   ultiple_rows=false\" to  │\n│        rate_code_id       ││     revert to previous    │\n│     store_and_fwd_flag    ││   behavior of returning a │\n│     pickup_location_id    ││ random row.')) ELSE #0 END│\n│    dropoff_location_id    ││                           │\n│        payment_type       ││                           │\n│        fare_amount        ││                           │\n│           extra           ││                           │\n│          mta_tax          ││                           │\n│         tip_amount        ││                           │\n│        tolls_amount       ││                           │\n│   improvement_surcharge   ││                           │\n│        total_amount       ││                           │\n│    congestion_surcharge   ││                           │\n│                           ││                           │\n│        7433139 Rows       ││           1 Rows          │\n│          (2.69s)          ││          (0.00s)          │\n└───────────────────────────┘└─────────────┬─────────────┘\n                             ┌─────────────┴─────────────┐\n                             │    UNGROUPED_AGGREGATE    │\n                             │    ────────────────────   │\n                             │        Aggregates:        │\n                             │        \"first\"(#0)        │\n                             │        count_star()       │\n                             │                           │\n                             │           1 Rows          │\n                             │          (0.00s)          │\n                             └─────────────┬─────────────┘\n                             ┌─────────────┴─────────────┐\n                             │         PROJECTION        │\n                             │    ────────────────────   │\n                             │             #0            │\n                             │                           │\n                             │           1 Rows          │\n                             │          (0.00s)          │\n                             └─────────────┬─────────────┘\n                             ┌─────────────┴─────────────┐\n                             │    UNGROUPED_AGGREGATE    │\n                             │    ────────────────────   │\n                             │        Aggregates:        │\n                             │         median(#0)        │\n                             │                           │\n                             │           1 Rows          │\n                             │          (0.17s)          │\n                             └─────────────┬─────────────┘\n                             ┌─────────────┴─────────────┐\n                             │         PROJECTION        │\n                             │    ────────────────────   │\n                             │         pickup_at         │\n                             │                           │\n                             │        7433139 Rows       │\n                             │          (0.00s)          │\n                             └─────────────┬─────────────┘\n                             ┌─────────────┴─────────────┐\n                             │         TABLE_SCAN        │\n                             │    ────────────────────   │\n                             │         Function:         │\n                             │        PARQUET_SCAN       │\n                             │                           │\n                             │        Projections:       │\n                             │         pickup_at         │\n                             │                           │\n                             │        7433139 Rows       │\n                             │          (0.20s)          │\n                             └───────────────────────────┘\nOur taxi table contains 7433139 rows, but the query scans the table twice! However, we would expect to see a full table scan to find the latest date and then a table scan that only scans a few rows or no rows at all. If you’re working with small datasets or overprovisioned servers, you probably wouldn’t notice this. For the longest time I didn’t! Now, as our team runs A LOT of queries, a slow query sticks out like a sore thumb.\nSo, what’s causing this behaviour? The problem is that the query engine doesn’t know the result of the subquery in the where clause. Because of this, it can’t project the where clause to the table, resulting in a full data scan. This is documented in BigQuery, MySQL, it replicates in Oracle, as well as DuckDB. Let’s fix this query!"
  },
  {
    "objectID": "posts/dbt-anti-pattern-using-this.html#new-header-just-in-case",
    "href": "posts/dbt-anti-pattern-using-this.html#new-header-just-in-case",
    "title": "testas",
    "section": "New header just in case",
    "text": "New header just in case\n\n\nselect * \nfrom events\nwhere event_at &gt; '2024-10-17'\n\n\n9 records\n\n\nid\nevent_at\n\n\n\n\n1\n2024-10-17 05:57:13\n\n\n2\n2024-10-19 19:01:32\n\n\n3\n2024-10-18 09:04:10\n\n\n5\n2024-10-19 00:57:16\n\n\n6\n2024-10-20 14:30:19\n\n\n7\n2024-10-18 12:40:32\n\n\n8\n2024-10-19 17:41:50\n\n\n9\n2024-10-18 18:50:10\n\n\n10\n2024-10-18 11:42:25"
  },
  {
    "objectID": "posts/dbt-anti-pattern-using-this.html#the-data",
    "href": "posts/dbt-anti-pattern-using-this.html#the-data",
    "title": "dbt anti-pattern: using {{ this }} directly in incremental refreshes",
    "section": "The Data",
    "text": "The Data\nI’m using DuckDB’s taxi dataset, specifically the 2019 04 partition as described here."
  },
  {
    "objectID": "posts/dbt-anti-pattern-using-this.html#fixing-the-query",
    "href": "posts/dbt-anti-pattern-using-this.html#fixing-the-query",
    "title": "dbt anti-pattern: using {{ this }} directly in incremental refreshes",
    "section": "Fixing the Query",
    "text": "Fixing the Query\nFixing the query requires us to separate the query into two queries. First, we need to save the latest date as a variable. Then, and only then, should we pass the variable to the final select statement. DuckDB has a useful feature called GETVARIABLE that allows us to do exactly that. dbt also supports variables, please refer to this StackOverflow answer. Here are benchmarks between a query that uses a subquery and one that first saves a variable:\n\nlibrary(duckdb)\nlibrary(dplyr)\n\ncon &lt;- dbConnect(duckdb::duckdb())\n\nfilter_on_variable &lt;- function() {\n\n  dbExecute(con, \"SET VARIABLE median_pickup_at = (select median(pickup_at) from 'taxi_2019_04.parquet')\")  \n  dbGetQuery(con, \"select * from 'taxi_2019_04.parquet' WHERE pickup_at &gt; GETVARIABLE('median_pickup_at')\")\n  \n}\n\nfilter_on_subquery &lt;- function() {\n\n  dbGetQuery(con, \"select * from 'taxi_2019_04.parquet' WHERE pickup_at &gt; (select median(pickup_at) from 'taxi_2019_04.parquet')\")\n  \n}\n\nread_benchmark &lt;- bench::mark(\n  min_iterations = 30,\n  filter_on_variable = filter_on_variable(),\n  filter_on_subquery = filter_on_subquery(),\n  check = TRUE\n)\n\nggplot2::autoplot(read_benchmark) + ggplot2::labs(title = \"Query run duration\", subtitle = \"Lower is better\")\n\nLoading required namespace: tidyr\n\n\n\n\n\n\n\n\nrm(read_benchmark)\n\nI know, the gain doesn’t seem THAT big. The difference diminishes if you’re running this in memory. But this is not a very large dataset and the query is straightforward. Furthermore, if you’re using BigQuery, the default dbt method scans more data and theoretically incurs unnecessary costs? I don’t use BQ, so I’m only surmising here. In real-world cases, these difference could be exasperated."
  },
  {
    "objectID": "posts/dbt-anti-pattern-pivot-macro.html",
    "href": "posts/dbt-anti-pattern-pivot-macro.html",
    "title": "dbt docs runs queries with pivot macros?",
    "section": "",
    "text": "I can’t for the life of me find this documented but apparently queries that use the pivot or unpivot macros are run so that their compiled code becomes available in the catalog.json file. It’s nuts that you have to run the query in order to have its code compiled – that’s just resources burnt into the air! And if the pivot query runs on large amounts of data, that also slows down your data warehouse. Jeez!"
  }
]